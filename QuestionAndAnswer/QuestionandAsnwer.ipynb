{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "if not os.path.exists('data'):\n",
        "    os.mkdir('/data')"
      ],
      "metadata": {
        "id": "7WcMD2qVqHqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/'\n",
        "res = requests.get(f'{url}train-v2.0.json')"
      ],
      "metadata": {
        "id": "eWBQAyzSqQg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in ['train-v2.0.json', 'dev-v2.0.json']:\n",
        "    res = requests.get(f'{url}{file}')\n",
        "    # write to file\n",
        "    with open(f'data/{file}', 'wb') as f:\n",
        "        for chunk in res.iter_content(chunk_size=4):\n",
        "            f.write(chunk)"
      ],
      "metadata": {
        "id": "HD2CX8u0ql53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Data Prep"
      ],
      "metadata": {
        "id": "NLi_GDtLuAvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_squad(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    # initialize lists for contexts, questions, and answers\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    # iterate through all data in squad data\n",
        "    #we are gonna iterate through the squad dataset and get the par, context and a data\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                if 'plausible_answers' in qa.keys():\n",
        "                    access = 'plausible_answers'\n",
        "                else:\n",
        "                    access = 'answers'\n",
        "                for answer in qa['answers']:\n",
        "                    # append data to lists\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "    # return formatted data lists\n",
        "    return contexts, questions, answers"
      ],
      "metadata": {
        "id": "AOstTC2oqsrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_contexts, train_questions, train_answers = read_squad('data/train-v2.0.json')\n",
        "val_contexts, val_questions, val_answers = read_squad('data/dev-v2.0.json')"
      ],
      "metadata": {
        "id": "FHGnWGEmqvEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding end token for bert tokenization\n",
        "\n",
        "Here we want to find the end token for each character, ideally it should be context strt + len of golden answer = text. However not always the case because someties the answer start is off by 1-2 indecies."
      ],
      "metadata": {
        "id": "FT32FlRUvDQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    # loop through each answer-context pair\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        # gold_text refers to the answer we are expecting to find in context\n",
        "        gold_text = answer['text']\n",
        "        # we already know the start index\n",
        "        start_idx = answer['answer_start']\n",
        "        # and ideally this would be the end index...\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # ...however, sometimes squad answers are off by a character or two\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            # if the answer is not off :)\n",
        "            answer['answer_end'] = end_idx\n",
        "        else:\n",
        "            for n in [1, 2]:\n",
        "                if context[start_idx-n:end_idx-n] == gold_text:\n",
        "                    # this means the answer is off by 'n' tokens\n",
        "                    answer['answer_start'] = start_idx - n\n",
        "                    answer['answer_end'] = end_idx - n"
      ],
      "metadata": {
        "id": "IT_lXGpQq0ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add endindex to train and validation dataset\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "metadata": {
        "id": "6ypqS2Gxq30d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize/Encode**"
      ],
      "metadata": {
        "id": "bosXk5anq6n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast #(smaller faster version of bert)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "#This will merge the 2 strings together context <pad> answer and this will be fed in our model|\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n"
      ],
      "metadata": {
        "id": "LJUPIyB6q574"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT expects 512 toekns to be fed in for everysample"
      ],
      "metadata": {
        "id": "60aGA_VIyB4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_positions(encodings, answers, rng):\n",
        "    # initialize lists to contain the token indices of answer start/end\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        # append start/end token position using char_to_token method\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        # end position cannot be found, char_to_token found space, so shift one token forward\n",
        "        go_back = 1\n",
        "        while end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end']-go_back)\n",
        "            go_back +=1\n",
        "    # update our encodings object with the new token-based start/end positions\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "# apply function to our data\n",
        "add_token_positions(train_encodings, train_answers,5000) ##change this back to len(train_answer) if you use full dataset\n",
        "add_token_positions(val_encodings, val_answers, 3000)"
      ],
      "metadata": {
        "id": "pSkddXWZq_Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to create a pytorch dataset object\n",
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self,idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "metadata": {
        "id": "RT9u02vnrQ0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "KsxCl7Et1KzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm #Traning bar\n"
      ],
      "metadata": {
        "id": "WUW0kYqK0bjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "fWkSJUS3-ZD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "device = torch.device('cuda')\n",
        "#modelp1 = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "modelp1 = DistilBertForQuestionAnswering.from_pretrained('model1')\n",
        "modelp1.to(device) #--temp disabled to train bi-LSTM\n",
        "modelp1.train()\n",
        "optim = AdamW(modelp1.parameters(), lr=5e-5)\n",
        "#print(modelp1.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEu-GsnG1X_i",
        "outputId": "84f58bd5-86e1-47b7-e1c3-4b6c388a099a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)"
      ],
      "metadata": {
        "id": "KWVSdsct2UhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(6):\n",
        "  loop = tqdm(train_loader)\n",
        "  for batch in loop:\n",
        "    optim.zero_grad() #We always want to reset our grad to zero\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "    outputs = modelp1(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "\n",
        "    loss = outputs[0]\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    optim.step()\n",
        "    # print relevant info to progress bar\n",
        "    loop.set_description(f'Epoch {epoch}')\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "_a2Xz5fr23P7",
        "outputId": "3d5a783b-3c86-4357-ff3b-54f1bfa695b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:   1%|          | 36/5000 [00:02<06:00, 13.76it/s, loss=3.82]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-31c7ce58da2e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# calculate loss for every parameter that needs grad update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'model1'\n",
        "modelp1.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57G16Acl4foA",
        "outputId": "4ff42bcf-28b6-4171-b57e-88bcaa9ed345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model1/tokenizer_config.json',\n",
              " 'model1/special_tokens_map.json',\n",
              " 'model1/vocab.txt',\n",
              " 'model1/added_tokens.json',\n",
              " 'model1/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To test BERT model (Optional Step)"
      ],
      "metadata": {
        "id": "1MxdT1osR6PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# switch model out of training mode\n",
        "modelp1.eval()\n",
        "\n",
        "#val_sampler = SequentialSampler(val_dataset)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "acc = []\n",
        "\n",
        "# initialize loop for progress bar\n",
        "loop = tqdm(val_loader)\n",
        "# loop through batches\n",
        "for batch in loop:\n",
        "    # we don't need to calculate gradients as we're not training\n",
        "    with torch.no_grad():\n",
        "        # pull batched items from loader\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        # make predictions\n",
        "        outputs = modelp1(input_ids, attention_mask=attention_mask)\n",
        "        # pull preds out\n",
        "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "        # calculate accuracy for both and append to accuracy list\n",
        "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "# calculate average accuracy in total\n",
        "acc = sum(acc)/len(acc)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "EEFqtL05O9KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "#When passing data to LSTM or GRU batch_size must be 1 or it wont work\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "def cls_pooling(model_output):\n",
        "  ans = []\n",
        "  for i in model_output:\n",
        "    ans.append(i[0][:,0])\n",
        "  return ans\n",
        "\n"
      ],
      "metadata": {
        "id": "5tdBp_4wsjqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM"
      ],
      "metadata": {
        "id": "uksZ34vpE3Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "\n",
        "class BERTLSTMModel(nn.Module):\n",
        "    def __init__(self, input_len, hidden_size, num_labels):\n",
        "        super(BERTLSTMModel, self).__init__()\n",
        "        #self.bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "        self.lstm = nn.LSTM(input_size=input_len,\n",
        "                            hidden_size=hidden_size,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True).to(device)\n",
        "        #used to find relevant span of text\n",
        "        self.fc_start = nn.Linear(hidden_size * 2, num_labels).to(device)\n",
        "        self.fc_end = nn.Linear(hidden_size * 2, num_labels).to(device)\n",
        "\n",
        "    #def forward(self, input_ids, bert_output):\n",
        "    def forward(self, end_logit):\n",
        "\n",
        "        lstm_out, _ = self.lstm(end_logit)\n",
        "        lstm_out_last = lstm_out[:, -1, :]\n",
        "\n",
        "        logits_start = self.fc_start(lstm_out_last)\n",
        "        logits_end = self.fc_end(lstm_out_last)\n",
        "        return logits_start, logits_end\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#distilmodel = modelp1\n",
        "#distilmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "distilmodel = DistilBertForQuestionAnswering.from_pretrained('modelp1')\n",
        "distilmodel.to(device) #--temp disabled to train bi-LSTM\n",
        "#distilmodel.train()\n",
        "#optim = AdamW(model.parameters(), lr=5e-5)\n",
        "total_loss = 0\n",
        "model = BERTLSTMModel(input_len=512,hidden_size=5, num_labels=(512))\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "total_loss = 0\n",
        "# Tokenize input\n",
        "#inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True)\n",
        "for epoch in range(1):\n",
        "  loop = tqdm(train_loader)\n",
        "  total_loss = 0\n",
        "  for batch in loop:\n",
        "    optimizer.zero_grad() #We always want to reset our grad to zero\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "    # outputs = distilmodel(input_ids, attention_mask=attention_mask,\n",
        "    #                     start_positions=start_positions,\n",
        "    #                     end_positions=end_positions)\n",
        "\n",
        "    outputs = distilmodel(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "\n",
        "\n",
        "    #sentence_embeddings = cls_pooling(outputs.start_logits)\n",
        "    #end_logits = sentence_embeddings\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    logits_start, logits_end = model(end_logits.unsqueeze(0))\n",
        "    # Compute loss for start positions\n",
        "    loss_start = criterion(logits_start, start_positions)\n",
        "\n",
        "    # Compute loss for end positions\n",
        "    loss_end = criterion(logits_end, end_positions)\n",
        "\n",
        "    # Total loss is the sum of start and end losses\n",
        "    total_loss += (loss_start + loss_end).item()\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    (loss_start + loss_end).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  average_loss = total_loss / len(train_loader)\n",
        "  print(f\"Loss: {average_loss}\")\n",
        "\n",
        "# model = BERTLSTMModel(hidden_size=128, num_labels=768)  # Replace num_labels with the actual number of classes\n",
        "# logits = model(train_loader['input_ids'], train_loader['attention_mask'])\n",
        "# print(logits)"
      ],
      "metadata": {
        "id": "BSgE-6i7w4QA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "bb119a1f-55cd-4a18-8d62-7df375f000f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            " 10%|█         | 520/5000 [00:30<04:22, 17.07it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-36cd97d413c6>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Total loss is the sum of start and end losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Backpropagation and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using GRU"
      ],
      "metadata": {
        "id": "iBWpqdBcE7i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "\n",
        "class BERTGRUModel(nn.Module):\n",
        "    def __init__(self, input_len, hidden_size, num_labels):\n",
        "        super(BERTGRUModel, self).__init__()\n",
        "        #self.bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "        self.lstm = nn.GRU(input_size=input_len,\n",
        "                            hidden_size=hidden_size,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True).to(device)\n",
        "        #used to find relevant span of text\n",
        "        self.fc_start = nn.Linear(hidden_size * 2, num_labels).to(device)\n",
        "        self.fc_end = nn.Linear(hidden_size * 2, num_labels).to(device)\n",
        "\n",
        "    #def forward(self, input_ids, bert_output):\n",
        "    def forward(self, end_logit):\n",
        "\n",
        "        lstm_out, _ = self.lstm(end_logit)\n",
        "        lstm_out_last = lstm_out[:, -1, :]\n",
        "\n",
        "        logits_start = self.fc_start(lstm_out_last)\n",
        "        logits_end = self.fc_end(lstm_out_last)\n",
        "        return logits_start, logits_end\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#distilmodel = modelp1\n",
        "#distilmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "distilmodel = DistilBertForQuestionAnswering.from_pretrained('modelp1')\n",
        "distilmodel.to(device) #--temp disabled to train bi-LSTM\n",
        "#distilmodel.train()\n",
        "#optim = AdamW(model.parameters(), lr=5e-5)\n",
        "total_loss = 0\n",
        "model = BERTGRUModel(input_len=512,hidden_size=50, num_labels=(512))\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "total_loss = 0\n",
        "# Tokenize input\n",
        "#inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True)\n",
        "for epoch in range(1):\n",
        "  loop = tqdm(train_loader)\n",
        "  total_loss = 0\n",
        "  for batch in loop:\n",
        "    optimizer.zero_grad() #We always want to reset our grad to zero\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "    # outputs = distilmodel(input_ids, attention_mask=attention_mask,\n",
        "    #                     start_positions=start_positions,\n",
        "    #                     end_positions=end_positions)\n",
        "\n",
        "    outputs = distilmodel(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "\n",
        "\n",
        "    #sentence_embeddings = cls_pooling(outputs.start_logits)\n",
        "    #end_logits = sentence_embeddings\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    logits_start, logits_end = model(end_logits.unsqueeze(0))\n",
        "    # Compute loss for start positions\n",
        "    loss_start = criterion(logits_start, start_positions)\n",
        "\n",
        "    # Compute loss for end positions\n",
        "    loss_end = criterion(logits_end, end_positions)\n",
        "\n",
        "    # Total loss is the sum of start and end losses\n",
        "    total_loss += (loss_start + loss_end).item()\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    (loss_start + loss_end).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  average_loss = total_loss / len(train_loader)\n",
        "  print(f\"Loss: {average_loss}\")\n",
        "\n",
        "# model = BERTLSTMModel(hidden_size=128, num_labels=768)  # Replace num_labels with the actual number of classes\n",
        "# logits = model(train_loader['input_ids'], train_loader['attention_mask'])\n",
        "# print(logits)"
      ],
      "metadata": {
        "id": "jLvhWGyjE7OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch model out of training mode\n",
        "model.eval()\n",
        "\n",
        "#val_sampler = SequentialSampler(val_dataset)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "acc = []\n",
        "\n",
        "# initialize loop for progress bar\n",
        "loop = tqdm(val_loader)\n",
        "i = 0\n",
        "# loop through batches\n",
        "for batch in loop:\n",
        "    # we don't need to calculate gradients as we're not training\n",
        "    with torch.no_grad():\n",
        "        i+=1\n",
        "        # pull batched items from loader\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        # make predictions\n",
        "        outputs = modelp1(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "\n",
        "        end_logits = outputs.end_logits\n",
        "        logits_start, logits_end = model(end_logits.unsqueeze(0))\n",
        "        # pull preds out\n",
        "        start_pred = torch.argmax(logits_start, dim=1)\n",
        "        # calculate accuracy for both and append to accuracy list\n",
        "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "\n",
        "\n",
        "# calculate average accuracy in total\n",
        "acc = sum(acc)/len(acc)\n",
        "\n",
        "print(acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXOFW8msOe5K",
        "outputId": "f0bb7651-d2ba-4938-c79a-fdb5afe24ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1269/1269 [06:02<00:00,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6432335641326117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ww17XuDeQkGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii3_lGFsQkrl",
        "outputId": "5fd7456a-b082-49dc-883a-9657c726b1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                          Version\n",
            "-------------------------------- ---------------------\n",
            "absl-py                          1.4.0\n",
            "aiohttp                          3.8.6\n",
            "aiosignal                        1.3.1\n",
            "alabaster                        0.7.13\n",
            "albumentations                   1.3.1\n",
            "altair                           4.2.2\n",
            "anyio                            3.7.1\n",
            "appdirs                          1.4.4\n",
            "argon2-cffi                      23.1.0\n",
            "argon2-cffi-bindings             21.2.0\n",
            "array-record                     0.5.0\n",
            "arviz                            0.15.1\n",
            "astropy                          5.3.4\n",
            "astunparse                       1.6.3\n",
            "async-timeout                    4.0.3\n",
            "atpublic                         4.0\n",
            "attrs                            23.1.0\n",
            "audioread                        3.0.1\n",
            "autograd                         1.6.2\n",
            "Babel                            2.13.1\n",
            "backcall                         0.2.0\n",
            "beautifulsoup4                   4.11.2\n",
            "bidict                           0.22.1\n",
            "bigframes                        0.13.0\n",
            "bleach                           6.1.0\n",
            "blinker                          1.4\n",
            "blis                             0.7.11\n",
            "blosc2                           2.0.0\n",
            "bokeh                            3.3.1\n",
            "bqplot                           0.12.42\n",
            "branca                           0.7.0\n",
            "build                            1.0.3\n",
            "CacheControl                     0.13.1\n",
            "cachetools                       5.3.2\n",
            "catalogue                        2.0.10\n",
            "certifi                          2023.7.22\n",
            "cffi                             1.16.0\n",
            "chardet                          5.2.0\n",
            "charset-normalizer               3.3.2\n",
            "chex                             0.1.7\n",
            "click                            8.1.7\n",
            "click-plugins                    1.1.1\n",
            "cligj                            0.7.2\n",
            "cloudpickle                      2.2.1\n",
            "cmake                            3.27.7\n",
            "cmdstanpy                        1.2.0\n",
            "colorcet                         3.0.1\n",
            "colorlover                       0.3.0\n",
            "colour                           0.1.5\n",
            "community                        1.0.0b1\n",
            "confection                       0.1.3\n",
            "cons                             0.4.6\n",
            "contextlib2                      21.6.0\n",
            "contourpy                        1.2.0\n",
            "cryptography                     41.0.5\n",
            "cufflinks                        0.17.3\n",
            "cupy-cuda11x                     11.0.0\n",
            "cvxopt                           1.3.2\n",
            "cvxpy                            1.3.2\n",
            "cycler                           0.12.1\n",
            "cymem                            2.0.8\n",
            "Cython                           3.0.5\n",
            "dask                             2023.8.1\n",
            "datascience                      0.17.6\n",
            "db-dtypes                        1.1.1\n",
            "dbus-python                      1.2.18\n",
            "debugpy                          1.6.6\n",
            "decorator                        4.4.2\n",
            "defusedxml                       0.7.1\n",
            "diskcache                        5.6.3\n",
            "distributed                      2023.8.1\n",
            "distro                           1.7.0\n",
            "dlib                             19.24.2\n",
            "dm-tree                          0.1.8\n",
            "docutils                         0.18.1\n",
            "dopamine-rl                      4.0.6\n",
            "duckdb                           0.9.2\n",
            "earthengine-api                  0.1.379\n",
            "easydict                         1.11\n",
            "ecos                             2.0.12\n",
            "editdistance                     0.6.2\n",
            "eerepr                           0.0.4\n",
            "en-core-web-sm                   3.6.0\n",
            "entrypoints                      0.4\n",
            "et-xmlfile                       1.1.0\n",
            "etils                            1.5.2\n",
            "etuples                          0.3.9\n",
            "exceptiongroup                   1.1.3\n",
            "fastai                           2.7.13\n",
            "fastcore                         1.5.29\n",
            "fastdownload                     0.0.7\n",
            "fastjsonschema                   2.19.0\n",
            "fastprogress                     1.0.3\n",
            "fastrlock                        0.8.2\n",
            "filelock                         3.13.1\n",
            "fiona                            1.9.5\n",
            "firebase-admin                   5.3.0\n",
            "Flask                            2.2.5\n",
            "flatbuffers                      23.5.26\n",
            "flax                             0.7.5\n",
            "folium                           0.14.0\n",
            "fonttools                        4.44.3\n",
            "frozendict                       2.3.8\n",
            "frozenlist                       1.4.0\n",
            "fsspec                           2023.6.0\n",
            "future                           0.18.3\n",
            "gast                             0.5.4\n",
            "gcsfs                            2023.6.0\n",
            "GDAL                             3.4.3\n",
            "gdown                            4.6.6\n",
            "geemap                           0.28.2\n",
            "gensim                           4.3.2\n",
            "geocoder                         1.38.1\n",
            "geographiclib                    2.0\n",
            "geopandas                        0.13.2\n",
            "geopy                            2.3.0\n",
            "gin-config                       0.5.0\n",
            "glob2                            0.7\n",
            "google                           2.0.3\n",
            "google-ai-generativelanguage     0.3.3\n",
            "google-api-core                  2.11.1\n",
            "google-api-python-client         2.84.0\n",
            "google-auth                      2.17.3\n",
            "google-auth-httplib2             0.1.1\n",
            "google-auth-oauthlib             1.0.0\n",
            "google-cloud-bigquery            3.12.0\n",
            "google-cloud-bigquery-connection 1.12.1\n",
            "google-cloud-bigquery-storage    2.22.0\n",
            "google-cloud-core                2.3.3\n",
            "google-cloud-datastore           2.15.2\n",
            "google-cloud-firestore           2.11.1\n",
            "google-cloud-functions           1.13.3\n",
            "google-cloud-iam                 2.12.2\n",
            "google-cloud-language            2.9.1\n",
            "google-cloud-resource-manager    1.10.4\n",
            "google-cloud-storage             2.8.0\n",
            "google-cloud-translate           3.11.3\n",
            "google-colab                     1.0.0\n",
            "google-crc32c                    1.5.0\n",
            "google-generativeai              0.2.2\n",
            "google-pasta                     0.2.0\n",
            "google-resumable-media           2.6.0\n",
            "googleapis-common-protos         1.61.0\n",
            "googledrivedownloader            0.4\n",
            "graphviz                         0.20.1\n",
            "greenlet                         3.0.1\n",
            "grpc-google-iam-v1               0.12.7\n",
            "grpcio                           1.59.2\n",
            "grpcio-status                    1.48.2\n",
            "gspread                          3.4.2\n",
            "gspread-dataframe                3.3.1\n",
            "gym                              0.25.2\n",
            "gym-notices                      0.0.8\n",
            "h5netcdf                         1.3.0\n",
            "h5py                             3.9.0\n",
            "holidays                         0.36\n",
            "holoviews                        1.17.1\n",
            "html5lib                         1.1\n",
            "httpimport                       1.3.1\n",
            "httplib2                         0.22.0\n",
            "huggingface-hub                  0.19.4\n",
            "humanize                         4.7.0\n",
            "hyperopt                         0.2.7\n",
            "ibis-framework                   6.2.0\n",
            "idna                             3.4\n",
            "imageio                          2.31.6\n",
            "imageio-ffmpeg                   0.4.9\n",
            "imagesize                        1.4.1\n",
            "imbalanced-learn                 0.10.1\n",
            "imgaug                           0.4.0\n",
            "importlib-metadata               6.8.0\n",
            "importlib-resources              6.1.1\n",
            "imutils                          0.5.4\n",
            "inflect                          7.0.0\n",
            "iniconfig                        2.0.0\n",
            "install                          1.3.5\n",
            "intel-openmp                     2023.2.0\n",
            "ipyevents                        2.0.2\n",
            "ipyfilechooser                   0.6.0\n",
            "ipykernel                        5.5.6\n",
            "ipyleaflet                       0.17.4\n",
            "ipython                          7.34.0\n",
            "ipython-genutils                 0.2.0\n",
            "ipython-sql                      0.5.0\n",
            "ipytree                          0.2.2\n",
            "ipywidgets                       7.7.1\n",
            "itsdangerous                     2.1.2\n",
            "jax                              0.4.20\n",
            "jaxlib                           0.4.20+cuda11.cudnn86\n",
            "jeepney                          0.7.1\n",
            "jieba                            0.42.1\n",
            "Jinja2                           3.1.2\n",
            "joblib                           1.3.2\n",
            "jsonpickle                       3.0.2\n",
            "jsonschema                       4.19.2\n",
            "jsonschema-specifications        2023.11.1\n",
            "jupyter-client                   6.1.12\n",
            "jupyter-console                  6.1.0\n",
            "jupyter_core                     5.5.0\n",
            "jupyter-server                   1.24.0\n",
            "jupyterlab-pygments              0.2.2\n",
            "jupyterlab-widgets               3.0.9\n",
            "kaggle                           1.5.16\n",
            "keras                            2.14.0\n",
            "keyring                          23.5.0\n",
            "kiwisolver                       1.4.5\n",
            "langcodes                        3.3.0\n",
            "launchpadlib                     1.10.16\n",
            "lazr.restfulclient               0.14.4\n",
            "lazr.uri                         1.0.6\n",
            "lazy_loader                      0.3\n",
            "libclang                         16.0.6\n",
            "librosa                          0.10.1\n",
            "lida                             0.0.10\n",
            "lightgbm                         4.1.0\n",
            "linkify-it-py                    2.0.2\n",
            "llmx                             0.0.15a0\n",
            "llvmlite                         0.41.1\n",
            "locket                           1.0.0\n",
            "logical-unification              0.4.6\n",
            "lxml                             4.9.3\n",
            "malloy                           2023.1064\n",
            "Markdown                         3.5.1\n",
            "markdown-it-py                   3.0.0\n",
            "MarkupSafe                       2.1.3\n",
            "matplotlib                       3.7.1\n",
            "matplotlib-inline                0.1.6\n",
            "matplotlib-venn                  0.11.9\n",
            "mdit-py-plugins                  0.4.0\n",
            "mdurl                            0.1.2\n",
            "miniKanren                       1.0.3\n",
            "missingno                        0.5.2\n",
            "mistune                          0.8.4\n",
            "mizani                           0.9.3\n",
            "mkl                              2023.2.0\n",
            "ml-dtypes                        0.2.0\n",
            "mlxtend                          0.22.0\n",
            "more-itertools                   10.1.0\n",
            "moviepy                          1.0.3\n",
            "mpmath                           1.3.0\n",
            "msgpack                          1.0.7\n",
            "multidict                        6.0.4\n",
            "multipledispatch                 1.0.0\n",
            "multitasking                     0.0.11\n",
            "murmurhash                       1.0.10\n",
            "music21                          9.1.0\n",
            "natsort                          8.4.0\n",
            "nbclassic                        1.0.0\n",
            "nbclient                         0.9.0\n",
            "nbconvert                        6.5.4\n",
            "nbformat                         5.9.2\n",
            "nest-asyncio                     1.5.8\n",
            "networkx                         3.2.1\n",
            "nibabel                          4.0.2\n",
            "nltk                             3.8.1\n",
            "notebook                         6.5.5\n",
            "notebook_shim                    0.2.3\n",
            "numba                            0.58.1\n",
            "numexpr                          2.8.7\n",
            "numpy                            1.23.5\n",
            "oauth2client                     4.1.3\n",
            "oauthlib                         3.2.2\n",
            "opencv-contrib-python            4.8.0.76\n",
            "opencv-python                    4.8.0.76\n",
            "opencv-python-headless           4.8.1.78\n",
            "openpyxl                         3.1.2\n",
            "opt-einsum                       3.3.0\n",
            "optax                            0.1.7\n",
            "orbax-checkpoint                 0.4.2\n",
            "osqp                             0.6.2.post8\n",
            "packaging                        23.2\n",
            "pandas                           1.5.3\n",
            "pandas-datareader                0.10.0\n",
            "pandas-gbq                       0.17.9\n",
            "pandas-stubs                     1.5.3.230304\n",
            "pandocfilters                    1.5.0\n",
            "panel                            1.3.1\n",
            "param                            2.0.1\n",
            "parso                            0.8.3\n",
            "parsy                            2.1\n",
            "partd                            1.4.1\n",
            "pathlib                          1.0.1\n",
            "pathy                            0.10.3\n",
            "patsy                            0.5.3\n",
            "peewee                           3.17.0\n",
            "pexpect                          4.8.0\n",
            "pickleshare                      0.7.5\n",
            "Pillow                           9.4.0\n",
            "pip                              23.1.2\n",
            "pip-tools                        6.13.0\n",
            "platformdirs                     4.0.0\n",
            "plotly                           5.15.0\n",
            "plotnine                         0.12.4\n",
            "pluggy                           1.3.0\n",
            "polars                           0.17.3\n",
            "pooch                            1.8.0\n",
            "portpicker                       1.5.2\n",
            "prefetch-generator               1.0.3\n",
            "preshed                          3.0.9\n",
            "prettytable                      3.9.0\n",
            "proglog                          0.1.10\n",
            "progressbar2                     4.2.0\n",
            "prometheus-client                0.18.0\n",
            "promise                          2.3\n",
            "prompt-toolkit                   3.0.41\n",
            "prophet                          1.1.5\n",
            "proto-plus                       1.22.3\n",
            "protobuf                         3.20.3\n",
            "psutil                           5.9.5\n",
            "psycopg2                         2.9.9\n",
            "ptyprocess                       0.7.0\n",
            "py-cpuinfo                       9.0.0\n",
            "py4j                             0.10.9.7\n",
            "pyarrow                          9.0.0\n",
            "pyasn1                           0.5.0\n",
            "pyasn1-modules                   0.3.0\n",
            "pycocotools                      2.0.7\n",
            "pycparser                        2.21\n",
            "pyct                             0.5.0\n",
            "pydantic                         1.10.13\n",
            "pydata-google-auth               1.8.2\n",
            "pydot                            1.4.2\n",
            "pydot-ng                         2.0.0\n",
            "pydotplus                        2.0.2\n",
            "PyDrive                          1.3.1\n",
            "PyDrive2                         1.6.3\n",
            "pyerfa                           2.0.1.1\n",
            "pygame                           2.5.2\n",
            "Pygments                         2.16.1\n",
            "PyGObject                        3.42.1\n",
            "PyJWT                            2.3.0\n",
            "pymc                             5.7.2\n",
            "pymystem3                        0.2.0\n",
            "PyOpenGL                         3.1.7\n",
            "pyOpenSSL                        23.3.0\n",
            "pyparsing                        3.1.1\n",
            "pyperclip                        1.8.2\n",
            "pyproj                           3.6.1\n",
            "pyproject_hooks                  1.0.0\n",
            "pyshp                            2.3.1\n",
            "PySocks                          1.7.1\n",
            "pytensor                         2.14.2\n",
            "pytest                           7.4.3\n",
            "python-apt                       0.0.0\n",
            "python-box                       7.1.1\n",
            "python-dateutil                  2.8.2\n",
            "python-louvain                   0.16\n",
            "python-slugify                   8.0.1\n",
            "python-utils                     3.8.1\n",
            "pytz                             2023.3.post1\n",
            "pyviz_comms                      3.0.0\n",
            "PyWavelets                       1.4.1\n",
            "PyYAML                           6.0.1\n",
            "pyzmq                            23.2.1\n",
            "qdldl                            0.1.7.post0\n",
            "qudida                           0.0.4\n",
            "ratelim                          0.1.6\n",
            "referencing                      0.31.0\n",
            "regex                            2023.6.3\n",
            "requests                         2.31.0\n",
            "requests-oauthlib                1.3.1\n",
            "requirements-parser              0.5.0\n",
            "rich                             13.7.0\n",
            "rpds-py                          0.13.0\n",
            "rpy2                             3.4.2\n",
            "rsa                              4.9\n",
            "safetensors                      0.4.0\n",
            "scikit-image                     0.19.3\n",
            "scikit-learn                     1.2.2\n",
            "scipy                            1.11.3\n",
            "scooby                           0.9.2\n",
            "scs                              3.2.4\n",
            "seaborn                          0.12.2\n",
            "SecretStorage                    3.3.1\n",
            "Send2Trash                       1.8.2\n",
            "setuptools                       67.7.2\n",
            "shapely                          2.0.2\n",
            "six                              1.16.0\n",
            "sklearn-pandas                   2.2.0\n",
            "smart-open                       6.4.0\n",
            "sniffio                          1.3.0\n",
            "snowballstemmer                  2.2.0\n",
            "sortedcontainers                 2.4.0\n",
            "soundfile                        0.12.1\n",
            "soupsieve                        2.5\n",
            "soxr                             0.3.7\n",
            "spacy                            3.6.1\n",
            "spacy-legacy                     3.0.12\n",
            "spacy-loggers                    1.0.5\n",
            "Sphinx                           5.0.2\n",
            "sphinxcontrib-applehelp          1.0.7\n",
            "sphinxcontrib-devhelp            1.0.5\n",
            "sphinxcontrib-htmlhelp           2.0.4\n",
            "sphinxcontrib-jsmath             1.0.1\n",
            "sphinxcontrib-qthelp             1.0.6\n",
            "sphinxcontrib-serializinghtml    1.1.9\n",
            "SQLAlchemy                       2.0.23\n",
            "sqlglot                          17.16.2\n",
            "sqlparse                         0.4.4\n",
            "srsly                            2.4.8\n",
            "stanio                           0.3.0\n",
            "statsmodels                      0.14.0\n",
            "sympy                            1.12\n",
            "tables                           3.8.0\n",
            "tabulate                         0.9.0\n",
            "tbb                              2021.11.0\n",
            "tblib                            3.0.0\n",
            "tenacity                         8.2.3\n",
            "tensorboard                      2.14.1\n",
            "tensorboard-data-server          0.7.2\n",
            "tensorflow                       2.14.0\n",
            "tensorflow-datasets              4.9.3\n",
            "tensorflow-estimator             2.14.0\n",
            "tensorflow-gcs-config            2.14.0\n",
            "tensorflow-hub                   0.15.0\n",
            "tensorflow-io-gcs-filesystem     0.34.0\n",
            "tensorflow-metadata              1.14.0\n",
            "tensorflow-probability           0.22.0\n",
            "tensorstore                      0.1.45\n",
            "termcolor                        2.3.0\n",
            "terminado                        0.18.0\n",
            "text-unidecode                   1.3\n",
            "textblob                         0.17.1\n",
            "tf-slim                          1.1.0\n",
            "thinc                            8.1.12\n",
            "threadpoolctl                    3.2.0\n",
            "tifffile                         2023.9.26\n",
            "tinycss2                         1.2.1\n",
            "tokenizers                       0.15.0\n",
            "toml                             0.10.2\n",
            "tomli                            2.0.1\n",
            "toolz                            0.12.0\n",
            "torch                            2.1.0+cu118\n",
            "torchaudio                       2.1.0+cu118\n",
            "torchdata                        0.7.0\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.16.0\n",
            "torchvision                      0.16.0+cu118\n",
            "tornado                          6.3.2\n",
            "tqdm                             4.66.1\n",
            "traitlets                        5.7.1\n",
            "traittypes                       0.2.1\n",
            "transformers                     4.35.2\n",
            "triton                           2.1.0\n",
            "tweepy                           4.14.0\n",
            "typer                            0.9.0\n",
            "types-pytz                       2023.3.1.1\n",
            "types-setuptools                 68.2.0.1\n",
            "typing_extensions                4.5.0\n",
            "tzlocal                          5.2\n",
            "uc-micro-py                      1.0.2\n",
            "uritemplate                      4.1.1\n",
            "urllib3                          2.0.7\n",
            "vega-datasets                    0.9.0\n",
            "wadllib                          1.3.6\n",
            "wasabi                           1.1.2\n",
            "wcwidth                          0.2.10\n",
            "webcolors                        1.13\n",
            "webencodings                     0.5.1\n",
            "websocket-client                 1.6.4\n",
            "Werkzeug                         3.0.1\n",
            "wheel                            0.41.3\n",
            "widgetsnbextension               3.6.6\n",
            "wordcloud                        1.9.2\n",
            "wrapt                            1.14.1\n",
            "xarray                           2023.7.0\n",
            "xarray-einstats                  0.6.0\n",
            "xgboost                          2.0.2\n",
            "xlrd                             2.0.1\n",
            "xxhash                           3.4.1\n",
            "xyzservices                      2023.10.1\n",
            "yarl                             1.9.2\n",
            "yellowbrick                      1.5\n",
            "yfinance                         0.2.31\n",
            "zict                             3.0.0\n",
            "zipp                             3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fz8cSq2eQlV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}